{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c313d3b0",
   "metadata": {},
   "source": [
    "# Latin-Amazigh Section (Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adbc56",
   "metadata": {},
   "source": [
    "## Data processing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import layers, models, optimizers, backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5f120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouad\\anaconda\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Function to load images with quality checks\n",
    "def load_images(image_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(image_folder):\n",
    "        class_folder = os.path.join(image_folder, label)\n",
    "        for filename in os.listdir(class_folder):\n",
    "            img_path = os.path.join(class_folder, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Convert to grayscale and resize\n",
    "                    img = img.convert('L').resize((28, 28))\n",
    "                    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                    # Quality control: Check for low contrast images\n",
    "                    if img_array.std() > 0.05: \n",
    "                        images.append(img_array.reshape(28, 28, 1))\n",
    "                        labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "    return np.array(images), labels\n",
    "\n",
    "\n",
    "latin_path = \"C:\\\\Users\\\\bouad\\\\OneDrive\\\\Bureau\\\\Amazigh NLP\\\\MNIST-BERBER\\\\Latin-version\\\\imgL\"\n",
    "latin_images, latin_labels = load_images(latin_path)\n",
    "encoder = LabelEncoder()\n",
    "latin_labels_encoded = encoder.fit_transform(latin_labels)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(latin_images, latin_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Training the model with augmented data\n",
    "train_generator = data_augmentation.flow(X_train, to_categorical(y_train), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a466f",
   "metadata": {},
   "source": [
    "## Capsul Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153d2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bouad\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 39ms/step - lambda_3_accuracy: 0.0348 - loss: nan - sequential_mse: 0.1116 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0872\n",
      "Epoch 2/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 39ms/step - lambda_3_accuracy: 0.0314 - loss: nan - sequential_mse: 0.0871 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 3/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 37ms/step - lambda_3_accuracy: 0.0309 - loss: nan - sequential_mse: 0.0871 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 4/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 38ms/step - lambda_3_accuracy: 0.0314 - loss: nan - sequential_mse: 0.0870 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 5/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 37ms/step - lambda_3_accuracy: 0.0311 - loss: nan - sequential_mse: 0.0871 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 6/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 36ms/step - lambda_3_accuracy: 0.0311 - loss: nan - sequential_mse: 0.0871 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 7/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 35ms/step - lambda_3_accuracy: 0.0320 - loss: nan - sequential_mse: 0.0871 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 8/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 37ms/step - lambda_3_accuracy: 0.0305 - loss: nan - sequential_mse: 0.0870 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 9/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 55ms/step - lambda_3_accuracy: 0.0306 - loss: nan - sequential_mse: 0.0872 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n",
      "Epoch 10/10\n",
      "\u001b[1m12400/12400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 44ms/step - lambda_3_accuracy: 0.0319 - loss: nan - sequential_mse: 0.0872 - val_lambda_3_accuracy: 0.0298 - val_loss: nan - val_sequential_mse: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26f7b441490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squash(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Squashing function to ensure that vectors have magnitudes between 0 and 1.\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(x), axis=axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * x\n",
    "\n",
    "def build_capsule_network(input_shape, num_classes):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layer to extract features\n",
    "    conv_layer = layers.Conv2D(64, (9, 9), strides=(1, 1), padding='valid', activation='relu')(input_layer)\n",
    "\n",
    "    primary_caps = layers.Conv2D(32, (9, 9), strides=(2, 2), padding='valid', activation='relu')(conv_layer)\n",
    "    primary_caps_shape = K.int_shape(primary_caps)\n",
    "    num_capsules = primary_caps_shape[1] * primary_caps_shape[2] * 32 // 8  # Calculate total number of capsules\n",
    "\n",
    "    # Reshaping \n",
    "    primary_caps = layers.Reshape((num_capsules, 8))(primary_caps)\n",
    "    primary_caps = layers.Lambda(lambda x: squash(x))(primary_caps)\n",
    "\n",
    "    digit_caps = layers.Dense(num_classes * 16, activation='relu')(primary_caps)  # Output size: [batch_size, num_capsules, num_classes * 16]\n",
    "\n",
    "    digit_caps = layers.Reshape((-1, num_classes, 16))(digit_caps)\n",
    "    output_caps = layers.Lambda(lambda x: squash(x))(digit_caps)\n",
    "\n",
    "    # Calculating the length of output capsules to match the true labels shape\n",
    "    y_pred = layers.Lambda(lambda z: K.sqrt(K.sum(K.square(z), axis=-1)))(output_caps)  # Output shape: (batch_size, num_capsules, num_classes)\n",
    "\n",
    "    # Reducing the dimensionality\n",
    "    y_pred = layers.Lambda(lambda x: K.max(x, axis=1))(y_pred)\n",
    "\n",
    "    # Add Decoder network for reconstruction\n",
    "    decoder_input = layers.Input(shape=(num_classes, 16))\n",
    "    flat_caps = layers.Flatten()(decoder_input)\n",
    "\n",
    "    num_elements = int(np.prod(input_shape))  # Convert to integer\n",
    "\n",
    "    decoder = models.Sequential([\n",
    "        layers.Input(shape=(num_classes * 16,)),  \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(num_elements, activation='sigmoid'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    decoder_output = decoder(flat_caps)\n",
    "\n",
    "    # Adding Full model with two inputs and two outputs\n",
    "    capsnet = models.Model(inputs=[input_layer, decoder_input], outputs=[y_pred, decoder_output])\n",
    "    capsnet.compile(optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "                    loss=['categorical_crossentropy', 'mse'], \n",
    "                    metrics=[['accuracy'], ['mse']])  \n",
    "\n",
    "    return capsnet\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 33  # Adjust to the number of classes in your dataset\n",
    "capsule_model = build_capsule_network(input_shape, num_classes)\n",
    "\n",
    "# Preparing the training data for the capsule network\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "\n",
    "decoder_input_data = np.zeros((X_train.shape[0], num_classes, 16))\n",
    "\n",
    "# Fitting the model using both inputs\n",
    "capsule_model.fit([X_train_reshaped, decoder_input_data], [y_train_categorical, X_train_reshaped], epochs=10, batch_size=16,\n",
    "                  validation_data=([X_test.reshape(X_test.shape[0], 28, 28, 1), np.zeros((X_test.shape[0], num_classes, 16))], \n",
    "                                   [to_categorical(y_test, num_classes), X_test.reshape(X_test.shape[0], 28, 28, 1)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a116e2",
   "metadata": {},
   "source": [
    "## Vision Transformer (VIT) Model Using EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caef54fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2267s\u001b[0m 340ms/step - accuracy: 0.6929 - loss: 1.1069 - val_accuracy: 0.9497 - val_loss: 0.1628\n",
      "Epoch 2/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2123s\u001b[0m 342ms/step - accuracy: 0.9330 - loss: 0.2655 - val_accuracy: 0.9716 - val_loss: 0.0978\n",
      "Epoch 3/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2114s\u001b[0m 341ms/step - accuracy: 0.9193 - loss: 0.2899 - val_accuracy: 0.8940 - val_loss: 0.3589\n",
      "Epoch 4/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2118s\u001b[0m 340ms/step - accuracy: 0.9159 - loss: 0.2976 - val_accuracy: 0.9444 - val_loss: 0.1904\n",
      "Epoch 5/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2243s\u001b[0m 359ms/step - accuracy: 0.8885 - loss: 0.3871 - val_accuracy: 0.9182 - val_loss: 0.2822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26ff651a5d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizing images to 32x32\n",
    "X_train_resized = tf.image.resize(X_train, [32, 32])\n",
    "X_test_resized = tf.image.resize(X_test, [32, 32])\n",
    "\n",
    "def build_vit_model(input_shape, num_classes):\n",
    "    # Using a pre-trained EfficientNetB0 model\n",
    "    base_model = EfficientNetB0(weights=None, include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "vit_model = build_vit_model((32, 32, 3), num_classes)\n",
    "vit_model.fit(X_train_resized, to_categorical(y_train), epochs=5, validation_data=(X_test_resized, to_categorical(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd9fc2",
   "metadata": {},
   "source": [
    "## LeNet-5 Model ( CNN ArchitectureType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce3f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouad\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 12ms/step - accuracy: 0.8631 - loss: 0.5034 - val_accuracy: 0.9660 - val_loss: 0.1180\n",
      "Epoch 2/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 12ms/step - accuracy: 0.9707 - loss: 0.0984 - val_accuracy: 0.9712 - val_loss: 0.0959\n",
      "Epoch 3/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 12ms/step - accuracy: 0.9780 - loss: 0.0752 - val_accuracy: 0.9733 - val_loss: 0.0931\n",
      "Epoch 4/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 14ms/step - accuracy: 0.9813 - loss: 0.0632 - val_accuracy: 0.9756 - val_loss: 0.0836\n",
      "Epoch 5/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 13ms/step - accuracy: 0.9830 - loss: 0.0565 - val_accuracy: 0.9784 - val_loss: 0.0728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26fed9d25d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_lenet5(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(6, (5, 5), activation='tanh', input_shape=input_shape, padding='same'),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)), \n",
    "        layers.Conv2D(16, (5, 5), activation='tanh'),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)), \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh'),\n",
    "        layers.Dense(84, activation='tanh'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lenet5_model = build_lenet5((28, 28, 1), num_classes)\n",
    "lenet5_model.fit(X_train_reshaped, y_train_categorical, epochs=5, \n",
    "                 validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b688749",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b01e244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouad\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 7ms/step - accuracy: 0.8425 - loss: 0.5657 - val_accuracy: 0.9430 - val_loss: 0.1958\n",
      "Epoch 2/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - accuracy: 0.9531 - loss: 0.1594 - val_accuracy: 0.9558 - val_loss: 0.1464\n",
      "Epoch 3/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.9653 - loss: 0.1181 - val_accuracy: 0.9637 - val_loss: 0.1263\n",
      "Epoch 4/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - accuracy: 0.9696 - loss: 0.0998 - val_accuracy: 0.9665 - val_loss: 0.1176\n",
      "Epoch 5/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0875 - val_accuracy: 0.9682 - val_loss: 0.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27019435f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_mlp(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mlp_model = build_mlp((28, 28, 1), num_classes)\n",
    "mlp_model.fit(X_train_reshaped, y_train_categorical, epochs=5, \n",
    "              validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a0eb1",
   "metadata": {},
   "source": [
    "## VGG-Like Small Network (CNN Architecture Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c649c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 51ms/step - accuracy: 0.9231 - loss: 0.2768 - val_accuracy: 0.9845 - val_loss: 0.0526\n",
      "Epoch 2/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 54ms/step - accuracy: 0.9868 - loss: 0.0442 - val_accuracy: 0.9857 - val_loss: 0.0450\n",
      "Epoch 3/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 51ms/step - accuracy: 0.9912 - loss: 0.0280 - val_accuracy: 0.9894 - val_loss: 0.0354\n",
      "Epoch 4/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 51ms/step - accuracy: 0.9939 - loss: 0.0193 - val_accuracy: 0.9884 - val_loss: 0.0414\n",
      "Epoch 5/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 53ms/step - accuracy: 0.9950 - loss: 0.0157 - val_accuracy: 0.9897 - val_loss: 0.0440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27019457310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vgg_like(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "vgg_like_model = build_vgg_like((28, 28, 1), num_classes)\n",
    "vgg_like_model.fit(X_train_reshaped, y_train_categorical, epochs=5, \n",
    "                   validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f3c75",
   "metadata": {},
   "source": [
    "## ResNet (Residual Network) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de07f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 69ms/step - accuracy: 0.9218 - loss: 0.2812 - val_accuracy: 0.9840 - val_loss: 0.0560\n",
      "Epoch 2/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 69ms/step - accuracy: 0.9867 - loss: 0.0466 - val_accuracy: 0.9876 - val_loss: 0.0425\n",
      "Epoch 3/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 71ms/step - accuracy: 0.9906 - loss: 0.0319 - val_accuracy: 0.9871 - val_loss: 0.0441\n",
      "Epoch 4/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 68ms/step - accuracy: 0.9924 - loss: 0.0233 - val_accuracy: 0.9892 - val_loss: 0.0412\n",
      "Epoch 5/5\n",
      "\u001b[1m6200/6200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 71ms/step - accuracy: 0.9942 - loss: 0.0178 - val_accuracy: 0.9862 - val_loss: 0.0485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x270188f98d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def residual_block(x, filters):\n",
    "    \"\"\"\n",
    "    A residual block that adds a shortcut connection to the output of two convolutional layers.\n",
    "    If the number of filters changes, a 1x1 convolution is applied to the shortcut to match the shape.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # Main path\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet_small(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # Adding residual blocks\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = residual_block(x, 64) \n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Flattening and adding dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Building the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "resnet_model = build_resnet_small((28, 28, 1), num_classes)\n",
    "resnet_model.fit(X_train_reshaped, y_train_categorical, epochs=5, \n",
    "                 validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd901e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
