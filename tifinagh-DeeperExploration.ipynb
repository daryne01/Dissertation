{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa0f9b3",
   "metadata": {},
   "source": [
    "# Tiginagh Section (Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd256085",
   "metadata": {},
   "source": [
    "## Data processing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eba318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images with quality checks\n",
    "def load_images(image_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(image_folder):\n",
    "        class_folder = os.path.join(image_folder, label)\n",
    "        for filename in os.listdir(class_folder):\n",
    "            img_path = os.path.join(class_folder, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Convert to grayscale and resize\n",
    "                    img = img.convert('L').resize((28, 28))\n",
    "                    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                    # Quality control: Check for low contrast images\n",
    "                    if img_array.std() > 0.05: \n",
    "                        images.append(img_array.reshape(28, 28, 1))\n",
    "                        labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "    return np.array(images), labels\n",
    "\n",
    "\n",
    "tifinagh_path = \"C:/Users/bouad/OneDrive/Bureau/Amazigh NLP/MNIST-BERBER/Tifinagh-version/imgT\"\n",
    "tifinagh_images, tifinagh_labels = load_images(tifinagh_path)\n",
    "encoder = LabelEncoder()\n",
    "tifinagh_labels_encoded = encoder.fit_transform(tifinagh_labels)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tifinagh_images, tifinagh_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Training the model with augmented data\n",
    "train_generator = data_augmentation.flow(X_train, to_categorical(y_train), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685301c8",
   "metadata": {},
   "source": [
    "## Capsul Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bda1fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 69ms/step - lambda_11_accuracy: 0.0330 - loss: nan - sequential_2_mse: 0.1692 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0906\n",
      "Epoch 2/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - lambda_11_accuracy: 0.0303 - loss: nan - sequential_2_mse: 0.0827 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0682\n",
      "Epoch 3/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - lambda_11_accuracy: 0.0288 - loss: nan - sequential_2_mse: 0.0671 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0631\n",
      "Epoch 4/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - lambda_11_accuracy: 0.0300 - loss: nan - sequential_2_mse: 0.0630 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0617\n",
      "Epoch 5/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - lambda_11_accuracy: 0.0301 - loss: nan - sequential_2_mse: 0.0623 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0611\n",
      "Epoch 6/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 61ms/step - lambda_11_accuracy: 0.0313 - loss: nan - sequential_2_mse: 0.0619 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0609\n",
      "Epoch 7/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 65ms/step - lambda_11_accuracy: 0.0322 - loss: nan - sequential_2_mse: 0.0614 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0608\n",
      "Epoch 8/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 67ms/step - lambda_11_accuracy: 0.0293 - loss: nan - sequential_2_mse: 0.0616 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0607\n",
      "Epoch 9/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 70ms/step - lambda_11_accuracy: 0.0290 - loss: nan - sequential_2_mse: 0.0614 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0607\n",
      "Epoch 10/10\n",
      "\u001b[1m1287/1287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - lambda_11_accuracy: 0.0306 - loss: nan - sequential_2_mse: 0.0613 - val_lambda_11_accuracy: 0.0293 - val_loss: nan - val_sequential_2_mse: 0.0607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x288d1d0cdd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squash(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Squashing function to ensure that vectors have magnitudes between 0 and 1.\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(x), axis=axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * x\n",
    "\n",
    "def build_capsule_network(input_shape, num_classes):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layer to extract features\n",
    "    conv_layer = layers.Conv2D(64, (9, 9), strides=(1, 1), padding='valid', activation='relu')(input_layer)\n",
    "\n",
    "    primary_caps = layers.Conv2D(32, (9, 9), strides=(2, 2), padding='valid', activation='relu')(conv_layer)\n",
    "    primary_caps_shape = K.int_shape(primary_caps)\n",
    "    num_capsules = primary_caps_shape[1] * primary_caps_shape[2] * 32 // 8 \n",
    "\n",
    "    # Reshaping\n",
    "    primary_caps = layers.Reshape((num_capsules, 8))(primary_caps)\n",
    "    primary_caps = layers.Lambda(lambda x: squash(x))(primary_caps)\n",
    "\n",
    "    digit_caps = layers.Dense(num_classes * 16, activation='relu')(primary_caps)  \n",
    "    digit_caps = layers.Reshape((-1, num_classes, 16))(digit_caps)\n",
    "    output_caps = layers.Lambda(lambda x: squash(x))(digit_caps)\n",
    "\n",
    "    # Calculating the length of output capsules to match the true labels shape\n",
    "    y_pred = layers.Lambda(lambda z: K.sqrt(K.sum(K.square(z), axis=-1)))(output_caps)  \n",
    "\n",
    "    # Reducing the dimensionality \n",
    "    y_pred = layers.Lambda(lambda x: K.max(x, axis=1))(y_pred)\n",
    "\n",
    "    # Add Decoder network for reconstruction\n",
    "    decoder_input = layers.Input(shape=(num_classes, 16))\n",
    "    flat_caps = layers.Flatten()(decoder_input)\n",
    "\n",
    "    num_elements = int(np.prod(input_shape))  # Convert to integer\n",
    "\n",
    "    decoder = models.Sequential([\n",
    "        layers.Input(shape=(num_classes * 16,)),  \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(num_elements, activation='sigmoid'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    decoder_output = decoder(flat_caps)\n",
    "\n",
    "    # Adding Full model with two inputs and two outputs\n",
    "    capsnet = models.Model(inputs=[input_layer, decoder_input], outputs=[y_pred, decoder_output])\n",
    "    capsnet.compile(optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "                    loss=['categorical_crossentropy', 'mse'], \n",
    "                    metrics=[['accuracy'], ['mse']])  \n",
    "\n",
    "    return capsnet\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 33  \n",
    "capsule_model = build_capsule_network(input_shape, num_classes)\n",
    "\n",
    "# Preparing the training data for the capsule network\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "\n",
    "decoder_input_data = np.zeros((X_train.shape[0], num_classes, 16))\n",
    "\n",
    "# Fitting the model using both inputs\n",
    "capsule_model.fit([X_train_reshaped, decoder_input_data], [y_train_categorical, X_train_reshaped], epochs=10, batch_size=16,\n",
    "                  validation_data=([X_test.reshape(X_test.shape[0], 28, 28, 1), np.zeros((X_test.shape[0], num_classes, 16))], \n",
    "                                   [to_categorical(y_test, num_classes), X_test.reshape(X_test.shape[0], 28, 28, 1)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c9346",
   "metadata": {},
   "source": [
    "## Vision Transformer (VIT) Model Using EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1852b066",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 263ms/step - accuracy: 0.1887 - loss: 2.9656 - val_accuracy: 0.1591 - val_loss: 3.8611\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 300ms/step - accuracy: 0.7482 - loss: 0.8506 - val_accuracy: 0.7900 - val_loss: 0.7541\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 302ms/step - accuracy: 0.8482 - loss: 0.5106 - val_accuracy: 0.8438 - val_loss: 0.5620\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 302ms/step - accuracy: 0.8767 - loss: 0.4390 - val_accuracy: 0.9147 - val_loss: 0.2980\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 323ms/step - accuracy: 0.8768 - loss: 0.4459 - val_accuracy: 0.1626 - val_loss: 5.7273\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 352ms/step - accuracy: 0.8459 - loss: 0.5357 - val_accuracy: 0.9196 - val_loss: 0.2485\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 350ms/step - accuracy: 0.8751 - loss: 0.4113 - val_accuracy: 0.9567 - val_loss: 0.1771\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 340ms/step - accuracy: 0.9157 - loss: 0.2955 - val_accuracy: 0.8654 - val_loss: 0.4498\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 351ms/step - accuracy: 0.9176 - loss: 0.2984 - val_accuracy: 0.9108 - val_loss: 0.2469\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 347ms/step - accuracy: 0.9086 - loss: 0.3118 - val_accuracy: 0.9586 - val_loss: 0.1406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x250cf4b6850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizing images to 32x32\n",
    "X_train_resized = tf.image.resize(X_train, [32, 32])\n",
    "X_test_resized = tf.image.resize(X_test, [32, 32])\n",
    "\n",
    "def build_vit_model(input_shape, num_classes):\n",
    "    # Using a pre-trained EfficientNetB0 model\n",
    "    base_model = EfficientNetB0(weights=None, include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "vit_model = build_vit_model((32, 32, 3), num_classes)\n",
    "vit_model.fit(X_train_resized, to_categorical(y_train), epochs=10, validation_data=(X_test_resized, to_categorical(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575808e4",
   "metadata": {},
   "source": [
    "## LeNet-5 Model ( CNN ArchitectureType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9e7b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.7122 - loss: 1.2352 - val_accuracy: 0.9446 - val_loss: 0.2247\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.9474 - loss: 0.1945 - val_accuracy: 0.9586 - val_loss: 0.1545\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9703 - loss: 0.1153 - val_accuracy: 0.9646 - val_loss: 0.1223\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9852 - loss: 0.0676 - val_accuracy: 0.9662 - val_loss: 0.1115\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9878 - loss: 0.0521 - val_accuracy: 0.9718 - val_loss: 0.0990\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9916 - loss: 0.0361 - val_accuracy: 0.9724 - val_loss: 0.0961\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9941 - loss: 0.0268 - val_accuracy: 0.9713 - val_loss: 0.0939\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9961 - loss: 0.0184 - val_accuracy: 0.9744 - val_loss: 0.0886\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.9980 - loss: 0.0116 - val_accuracy: 0.9755 - val_loss: 0.0888\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9970 - loss: 0.0130 - val_accuracy: 0.9734 - val_loss: 0.0907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25083ab6d10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_lenet5(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(6, (5, 5), activation='tanh', input_shape=input_shape, padding='same'),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),  \n",
    "        layers.Conv2D(16, (5, 5), activation='tanh'),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),  \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh'),\n",
    "        layers.Dense(84, activation='tanh'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lenet5_model = build_lenet5((28, 28, 1), num_classes)\n",
    "lenet5_model.fit(X_train_reshaped, y_train_categorical, epochs=10, \n",
    "                 validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa10773",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e226fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouad\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.6475 - loss: 1.4157 - val_accuracy: 0.9031 - val_loss: 0.3249\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9297 - loss: 0.2532 - val_accuracy: 0.9293 - val_loss: 0.2369\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1592 - val_accuracy: 0.9328 - val_loss: 0.2202\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.1133 - val_accuracy: 0.9431 - val_loss: 0.1942\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.0775 - val_accuracy: 0.9398 - val_loss: 0.2050\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9846 - loss: 0.0594 - val_accuracy: 0.9443 - val_loss: 0.1919\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0388 - val_accuracy: 0.9386 - val_loss: 0.2237\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9918 - loss: 0.0335 - val_accuracy: 0.9468 - val_loss: 0.2114\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9938 - loss: 0.0247 - val_accuracy: 0.9411 - val_loss: 0.2466\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9941 - loss: 0.0227 - val_accuracy: 0.9326 - val_loss: 0.2877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x250fc3693d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_mlp(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mlp_model = build_mlp((28, 28, 1), num_classes)\n",
    "mlp_model.fit(X_train_reshaped, y_train_categorical, epochs=10, \n",
    "              validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7bd42b",
   "metadata": {},
   "source": [
    "## VGG-Like Small Network (CNN Architecture Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a0b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 53ms/step - accuracy: 0.7963 - loss: 0.7330 - val_accuracy: 0.9736 - val_loss: 0.0907\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 52ms/step - accuracy: 0.9833 - loss: 0.0563 - val_accuracy: 0.9792 - val_loss: 0.0660\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 51ms/step - accuracy: 0.9894 - loss: 0.0325 - val_accuracy: 0.9823 - val_loss: 0.0610\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.9917 - loss: 0.0273 - val_accuracy: 0.9858 - val_loss: 0.0498\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 55ms/step - accuracy: 0.9977 - loss: 0.0091 - val_accuracy: 0.9800 - val_loss: 0.0776\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 52ms/step - accuracy: 0.9943 - loss: 0.0168 - val_accuracy: 0.9858 - val_loss: 0.0424\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 53ms/step - accuracy: 0.9965 - loss: 0.0110 - val_accuracy: 0.9823 - val_loss: 0.0603\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.9965 - loss: 0.0111 - val_accuracy: 0.9882 - val_loss: 0.0440\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 46ms/step - accuracy: 0.9987 - loss: 0.0056 - val_accuracy: 0.9848 - val_loss: 0.0529\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 48ms/step - accuracy: 0.9955 - loss: 0.0124 - val_accuracy: 0.9841 - val_loss: 0.0581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x250fd6c1150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vgg_like(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "vgg_like_model = build_vgg_like((28, 28, 1), num_classes)\n",
    "vgg_like_model.fit(X_train_reshaped, y_train_categorical, epochs=10, \n",
    "                   validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8593a6",
   "metadata": {},
   "source": [
    "## ResNet (Residual Network) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dcd30a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 68ms/step - accuracy: 0.7896 - loss: 0.7609 - val_accuracy: 0.9738 - val_loss: 0.0957\n",
      "Epoch 2/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 66ms/step - accuracy: 0.9812 - loss: 0.0700 - val_accuracy: 0.9792 - val_loss: 0.0682\n",
      "Epoch 3/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 72ms/step - accuracy: 0.9881 - loss: 0.0359 - val_accuracy: 0.9780 - val_loss: 0.0792\n",
      "Epoch 4/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.9926 - loss: 0.0295 - val_accuracy: 0.9883 - val_loss: 0.0466\n",
      "Epoch 5/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 64ms/step - accuracy: 0.9930 - loss: 0.0234 - val_accuracy: 0.9782 - val_loss: 0.0801\n",
      "Epoch 6/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9962 - loss: 0.0149 - val_accuracy: 0.9864 - val_loss: 0.0580\n",
      "Epoch 7/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9970 - loss: 0.0121 - val_accuracy: 0.9862 - val_loss: 0.0585\n",
      "Epoch 8/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.9964 - loss: 0.0107 - val_accuracy: 0.9883 - val_loss: 0.0524\n",
      "Epoch 9/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9980 - loss: 0.0071 - val_accuracy: 0.9880 - val_loss: 0.0393\n",
      "Epoch 10/10\n",
      "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9876 - val_loss: 0.0463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x250ffeb6d10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def residual_block(x, filters):\n",
    "    \"\"\"\n",
    "    A residual block that adds a shortcut connection to the output of two convolutional layers.\n",
    "    If the number of filters changes, a 1x1 convolution is applied to the shortcut to match the shape.\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # Main path\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet_small(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # Adding residual blocks\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = residual_block(x, 64)  \n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Flattening and adding dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Builduing the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "resnet_model = build_resnet_small((28, 28, 1), num_classes)\n",
    "resnet_model.fit(X_train_reshaped, y_train_categorical, epochs=10, \n",
    "                 validation_data=(X_test.reshape(X_test.shape[0], 28, 28, 1), to_categorical(y_test, num_classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b36066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
